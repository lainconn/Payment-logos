{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10192974,"sourceType":"datasetVersion","datasetId":6297924},{"sourceId":199676,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":170320,"modelId":192640}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate\n\nimport pandas as pd\nfrom collections import Counter\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport transformers\nfrom transformers import (\n    CLIPProcessor,\n    CLIPModel\n)\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom PIL import Image\nfrom tqdm import tqdm\nimport evaluate\nimport ast\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:03:36.751606Z","iopub.execute_input":"2024-12-16T02:03:36.752122Z","iopub.status.idle":"2024-12-16T02:04:04.311726Z","shell.execute_reply.started":"2024-12-16T02:03:36.752090Z","shell.execute_reply":"2024-12-16T02:04:04.311003Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"TRAIN_CSV = \"/kaggle/input/simplified-task/train_data.csv\"\nVAL_CSV = \"/kaggle/input/simplified-task/validation_data.csv\"\nTR_IMAGE_DIR = \"/kaggle/input/simplified-task/Dataset/Dataset/Dataset/Train/\"\nVAL_IMAGE_DIR = \"/kaggle/input/simplified-task/Dataset/Dataset/Dataset/Validation/\"\nTEACHER_PATH = \"openai/clip-vit-base-patch32\"\nBATCH_SIZE = 1\nNUM_CLASSES = 4\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:09:31.761518Z","iopub.execute_input":"2024-12-16T02:09:31.762240Z","iopub.status.idle":"2024-12-16T02:09:31.767009Z","shell.execute_reply.started":"2024-12-16T02:09:31.762206Z","shell.execute_reply":"2024-12-16T02:09:31.766080Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_CSV)\nval_df = pd.read_csv(VAL_CSV)\n\nprint(\"Unique values for Train: \" + str({**Counter(train_df.text)}))\nprint(\"Unique values for Validation: \" + str({**Counter(val_df.text)}))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:04:17.313943Z","iopub.execute_input":"2024-12-16T02:04:17.314331Z","iopub.status.idle":"2024-12-16T02:04:17.352701Z","shell.execute_reply.started":"2024-12-16T02:04:17.314298Z","shell.execute_reply":"2024-12-16T02:04:17.351798Z"}},"outputs":[{"name":"stdout","text":"Unique values for Train: {'Belkart': 216, 'Mastercard': 213, 'Mir': 202, 'Visa Mastercard Mir': 226, 'Visa': 213, 'Visa Mastercard Belkart': 200, 'Mastercard Belkart Mir': 201, 'Visa Mir': 201, 'Accept': 190, 'Belkart Password': 201, 'ID-Check Belkart Password': 201, 'ID-Check Belkart': 200, 'ID-Check Mastercard': 201, 'ID-Check': 198, 'Mir Accept': 201, 'Password': 198, 'Secure ID-Check': 201, 'Secure': 202, 'Visa Secure': 201, 'Other': 201}\nUnique values for Validation: {'Visa Mastercard Mir': 101, 'Visa Mastercard': 100, 'Visa Mastercard Belkart': 200, 'ID-Check Accept Secure Password': 101, 'ID-Check Secure Password': 101}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train_df = train_df[(train_df.text == \"Visa Mir\") | \n                    (train_df.text == \"Visa\") | \n                    (train_df.text == \"Other\") |\n                    (train_df.text == \"Mir\") | \n                    (train_df.text == \"Mastercard\") |\n                    (train_df.text == \"Visa Mastercard Mir\")]\n\nval_df = val_df[(val_df.text == \"Visa Mastercard Mir\") |\n                (val_df.text == \"Visa Mastercard\") |\n                (val_df.text == \"Visa Mastercard Belkart\")]\n\ntrain_df.reset_index(drop=True, inplace=True)\nval_df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:04:19.681611Z","iopub.execute_input":"2024-12-16T02:04:19.682479Z","iopub.status.idle":"2024-12-16T02:04:19.695866Z","shell.execute_reply.started":"2024-12-16T02:04:19.682428Z","shell.execute_reply":"2024-12-16T02:04:19.694908Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_df.loc[:, \"labels\"] = train_df.labels.apply(ast.literal_eval)\ntrain_df.loc[:, \"labels\"] = train_df.labels.apply(np.float32)\n\nval_df.loc[:, \"labels\"] = val_df.labels.apply(ast.literal_eval)\nval_df.loc[:, \"labels\"] = val_df.labels.apply(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:04:27.802838Z","iopub.execute_input":"2024-12-16T02:04:27.803560Z","iopub.status.idle":"2024-12-16T02:04:27.831588Z","shell.execute_reply.started":"2024-12-16T02:04:27.803522Z","shell.execute_reply":"2024-12-16T02:04:27.830882Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Loader(Dataset):\n    def __init__(self, data_dir, df, processor):\n        super().__init__()\n        self.data_dir = data_dir\n        self.df = df\n        self.processor = processor\n\n    \n    def __len__(self):\n        return len(self.df)\n\n\n    def __getitem__(self, index):\n        file_name = self.df[\"file_name\"][index]\n        labels = self.df[\"labels\"][index]\n        image = Image.open(self.data_dir + file_name).convert(\"RGBA\")\n        pixel_values = self.processor.image_processor(image, return_tensors=\"pt\").pixel_values\n        \n        return {\"pixel_values\": pixel_values.squeeze(),\n                \"labels\": torch.tensor(labels)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:04:31.416046Z","iopub.execute_input":"2024-12-16T02:04:31.416410Z","iopub.status.idle":"2024-12-16T02:04:31.422659Z","shell.execute_reply.started":"2024-12-16T02:04:31.416379Z","shell.execute_reply":"2024-12-16T02:04:31.421741Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class_list = [\n    \"Visa\",\n    \"Mastercard\",\n    \"Mir\",\n    \"Other\"\n]\n\nprompt_list = []\nfor item in class_list:\n    prompt = \"A photo of \" + item\n    prompt_list.append(prompt)\n    \nprocessor = CLIPProcessor.from_pretrained(TEACHER_PATH)\n\ntext = processor.tokenizer(prompt_list, padding=\"max_length\",\n                           truncation=True, return_tensors=\"pt\").to(device)\n\ntrain_dataset = Loader(data_dir=TR_IMAGE_DIR, df=train_df,\n                       processor=processor)\n\nval_dataset = Loader(data_dir=VAL_IMAGE_DIR, df=val_df,\n                     processor=processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:05:48.185427Z","iopub.execute_input":"2024-12-16T02:05:48.185828Z","iopub.status.idle":"2024-12-16T02:05:50.710741Z","shell.execute_reply.started":"2024-12-16T02:05:48.185796Z","shell.execute_reply":"2024-12-16T02:05:50.709985Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"095a1c6a4f1f4f6680accf5ab60d55a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e06c82aab25c4399bd83c18c90dbdaf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddad15af4c274c7aa0a9e8ba0bee144f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"430bd2b5571f498f9798b76efef11ddd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3667f94433c44820becc697dd4805968"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20f8b94bba6c4ba3912efdc74564307e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e4859778e5d4fab9ccd9f26be0c5dde"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, \n                              batch_size=BATCH_SIZE,\n                              drop_last=True,\n                              shuffle=True)\n\neval_dataloader = DataLoader(val_dataset, \n                             batch_size=BATCH_SIZE,\n                             drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:05:53.805788Z","iopub.execute_input":"2024-12-16T02:05:53.806153Z","iopub.status.idle":"2024-12-16T02:05:53.810973Z","shell.execute_reply.started":"2024-12-16T02:05:53.806126Z","shell.execute_reply":"2024-12-16T02:05:53.810041Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\"\"\"\"for param in CLIP.named_parameters():\n    if (param[0] == 'visual_projection.weight' or\n        param[0] == 'text_projection.weight' or\n        param[0] == \"logit_scale\"):\n        continue\n    else:\n        param[1].requires_grad=True\"\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T18:32:05.939022Z","iopub.execute_input":"2024-12-13T18:32:05.939446Z","iopub.status.idle":"2024-12-13T18:32:05.947130Z","shell.execute_reply.started":"2024-12-13T18:32:05.939410Z","shell.execute_reply":"2024-12-13T18:32:05.945741Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"def normalize(vector, e=1e-08):\n    return (vector - vector.mean())/(vector.std() + e)\n\nclass CustomModel(nn.Module):\n    def __init__(self, model, num_labels):\n        super(CustomModel,self).__init__()\n        self.num_labels = num_labels\n        self.model = model\n        self.classifier = nn.Linear(4,self.num_labels)\n        self.normalize = normalize\n        self.activation = nn.Sigmoid()\n    \n\n    def forward(self, input_ids, pixel_values, attention_mask, labels=None):\n\n        with torch.no_grad():\n            outputs = self.model(pixel_values=pixel_values,\n                                 input_ids=input_ids,\n                                 attention_mask=attention_mask)\n\n        logits_per_image = outputs.logits_per_image\n        flattened_logits = torch.flatten(logits_per_image)\n        #normalized_logits = self.normalize(flattened_logits)\n        logits = self.activation(self.classifier(flattened_logits))\n        \n        loss=None\n        if labels is not None:\n            criterion = nn.BCELoss()\n            loss = criterion(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n\n    \n        return SequenceClassifierOutput(loss=loss, logits=logits)\n\n\nstats = torch.load(\"/kaggle/input/clip_state/pytorch/default/1/state.pt\")\nCLIP = CLIPModel.from_pretrained(TEACHER_PATH,attn_implementation=\"sdpa\")\nCLIP.load_state_dict(stats)\nmodel = CustomModel(CLIP, NUM_CLASSES).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:30:32.936173Z","iopub.execute_input":"2024-12-16T02:30:32.936861Z","iopub.status.idle":"2024-12-16T02:30:34.218922Z","shell.execute_reply.started":"2024-12-16T02:30:32.936825Z","shell.execute_reply":"2024-12-16T02:30:34.218069Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/253330607.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  stats = torch.load(\"/kaggle/input/clip_state/pytorch/default/1/state.pt\")\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nf1 = evaluate.load(\"f1\", trust_remote_code=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:30:38.458597Z","iopub.execute_input":"2024-12-16T02:30:38.458997Z","iopub.status.idle":"2024-12-16T02:30:38.968040Z","shell.execute_reply.started":"2024-12-16T02:30:38.458966Z","shell.execute_reply":"2024-12-16T02:30:38.967296Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"#iter = []\npreds = []\n\nfor epoch in range(5):\n    train_loss = 0.0\n    model.train()\n    for batch in tqdm(train_dataloader):\n        for k,v in batch.items():\n            batch[k] = v.to(device)\n\n        outputs = model(pixel_values=batch[\"pixel_values\"],\n                       input_ids=text[\"input_ids\"],\n                       attention_mask=text[\"attention_mask\"],\n                       labels=batch[\"labels\"])\n        \n        loss = outputs.loss\n        loss.backward()\n        #plot_grad_flow(CLIP.named_parameters())\n        #iter.append(ave_grads)\n        optimizer.step()\n        optimizer.zero_grad()\n        train_loss += loss.item()\n\n    model.eval()\n    for batch in tqdm(eval_dataloader):\n        for k,v in batch.items():\n            batch[k] = v.to(device)\n\n        outputs = model(pixel_values=batch[\"pixel_values\"],\n                        input_ids=text[\"input_ids\"],\n                        attention_mask=text[\"attention_mask\"])\n\n        logits = outputs.logits\n        activation = torch.sigmoid(logits)\n        predictions = (activation >= 0.5).float()\n        preds.append(predictions)\n        f1.add_batch(predictions=predictions,\n                     references=batch[\"labels\"][0])\n\n        \n    print(\"Training Loss: \" + str(train_loss / len(train_dataloader)))\n    print(f1.compute(average='binary'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:31:43.600019Z","iopub.execute_input":"2024-12-16T02:31:43.600710Z","iopub.status.idle":"2024-12-16T02:35:41.838809Z","shell.execute_reply.started":"2024-12-16T02:31:43.600676Z","shell.execute_reply":"2024-12-16T02:35:41.837803Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1256/1256 [00:35<00:00, 35.21it/s]\n100%|██████████| 401/401 [00:11<00:00, 33.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 2.321382303682162\n{'f1': 0.814924270410048}\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1256/1256 [00:35<00:00, 35.69it/s]\n100%|██████████| 401/401 [00:11<00:00, 33.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.9732458575266847\n{'f1': 0.814924270410048}\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1256/1256 [00:35<00:00, 34.95it/s]\n100%|██████████| 401/401 [00:12<00:00, 33.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.499494560246777\n{'f1': 0.814924270410048}\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1256/1256 [00:35<00:00, 35.60it/s]\n100%|██████████| 401/401 [00:12<00:00, 32.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.42920639040245184\n{'f1': 0.814924270410048}\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1256/1256 [00:35<00:00, 34.95it/s]\n100%|██████████| 401/401 [00:12<00:00, 33.28it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.39423197793187037\n{'f1': 0.814924270410048}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"#pred = torch.cat(preds, dim=-1)\npred = torch.stack(preds)\ntrue = torch.tensor(val_df[\"labels\"], dtype=torch.float32)\n\nprint(sklearn.metrics.classification_report(\n    true.cpu().numpy(),\n    pred[-true.shape[0]:].cpu().numpy(),\n    target_names=[\"Mastercard\",\n                  \"Visa\",\n                  \"Мир\",\n                  \"Иные\"]\n))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:35:46.462137Z","iopub.execute_input":"2024-12-16T02:35:46.462524Z","iopub.status.idle":"2024-12-16T02:35:46.480616Z","shell.execute_reply.started":"2024-12-16T02:35:46.462494Z","shell.execute_reply":"2024-12-16T02:35:46.479765Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n  Mastercard       1.00      1.00      1.00       401\n        Visa       1.00      1.00      1.00       401\n         Мир       0.25      1.00      0.40       101\n        Иные       0.50      1.00      0.67       200\n\n   micro avg       0.69      1.00      0.81      1103\n   macro avg       0.69      1.00      0.77      1103\nweighted avg       0.84      1.00      0.88      1103\n samples avg       0.69      1.00      0.81      1103\n\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"image = train_dataset[414][\"pixel_values\"].unsqueeze(0).to(device)\ntext = processor.tokenizer(text=[\"a photo of Visa\",\"a photo of Mastercard\", \"a photo of Mir\",\n                                 \"a photo of Other\"],\n                          return_tensors=\"pt\",\n                          padding=True).to(device)\n\noutputs = CLIP(pixel_values=image, input_ids=text[\"input_ids\"])\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=-1)\nprint(torch.round(probs, decimals=3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:31:03.823082Z","iopub.execute_input":"2024-12-16T02:31:03.823712Z","iopub.status.idle":"2024-12-16T02:31:03.857454Z","shell.execute_reply.started":"2024-12-16T02:31:03.823678Z","shell.execute_reply":"2024-12-16T02:31:03.856582Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.0000, 0.0000, 0.9860, 0.0140]], device='cuda:0',\n       grad_fn=<RoundBackward1>)\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"train_df[train_df.text == \"Mir\"].index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:30:46.476155Z","iopub.execute_input":"2024-12-16T02:30:46.476638Z","iopub.status.idle":"2024-12-16T02:30:46.486977Z","shell.execute_reply.started":"2024-12-16T02:30:46.476593Z","shell.execute_reply":"2024-12-16T02:30:46.486098Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"Index([213, 214, 215, 216, 217, 218, 219, 220, 221, 222,\n       ...\n       405, 406, 407, 408, 409, 410, 411, 412, 413, 414],\n      dtype='int64', length=202)"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"image = train_dataset[414][\"pixel_values\"].unsqueeze(0).to(device)\ntext = processor.tokenizer(text=[\"A photo of Visa\",\"A photo of Mastercard\", \"A photo of Mir\",\n                                 \"A photo of Other\"],\n                          return_tensors=\"pt\",\n                          padding=True).to(device)\n\noutputs = model(pixel_values=image, \n                input_ids=text[\"input_ids\"],\n                attention_mask=text[\"attention_mask\"])\nlogits_per_image = outputs.logits\nprobs = logits_per_image.sigmoid()\nprint(torch.round(probs, decimals=3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:30:49.663215Z","iopub.execute_input":"2024-12-16T02:30:49.663565Z","iopub.status.idle":"2024-12-16T02:30:49.696553Z","shell.execute_reply.started":"2024-12-16T02:30:49.663533Z","shell.execute_reply":"2024-12-16T02:30:49.695789Z"}},"outputs":[{"name":"stdout","text":"tensor([0.5000, 0.5000, 0.5000, 0.5090], device='cuda:0',\n       grad_fn=<RoundBackward1>)\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"\"\"\"def plot_grad_flow(named_parameters):\n    global ave_grads\n    ave_grads = []\n    global layers\n    layers = []\n    for n, p in named_parameters:\n        if(p.requires_grad) and (\"bias\" not in n):\n            layers.append(n)\n            ave_grads.append(p.grad.abs().mean().cpu())\n    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n    plt.xlim(xmin=0, xmax=len(ave_grads))\n    plt.xlabel(\"Layers\")\n    plt.ylabel(\"average gradient\")\n    plt.title(\"Gradient flow\")\n    plt.grid(True)\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T18:32:11.528034Z","iopub.execute_input":"2024-12-13T18:32:11.528786Z","iopub.status.idle":"2024-12-13T18:32:11.535098Z","shell.execute_reply.started":"2024-12-13T18:32:11.528748Z","shell.execute_reply":"2024-12-13T18:32:11.534033Z"}},"outputs":[],"execution_count":102}]}