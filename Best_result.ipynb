{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10330453,"sourceType":"datasetVersion","datasetId":6386013}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate\n\nimport pandas as pd\nfrom collections import Counter\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport transformers\nfrom transformers import (\n    CLIPProcessor,\n    CLIPModel,\n    CLIPConfig,\n    CLIPTextConfig,\n    CLIPVisionConfig\n)\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom PIL import Image, ImageOps\nfrom tqdm.notebook import tqdm\nimport evaluate\nimport ast\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:47:32.742890Z","iopub.execute_input":"2025-01-03T05:47:32.743856Z","iopub.status.idle":"2025-01-03T05:48:01.458192Z","shell.execute_reply.started":"2025-01-03T05:47:32.743800Z","shell.execute_reply":"2025-01-03T05:48:01.457225Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"TRAIN_CSV = \"/kaggle/input/final-version-dataset/train_data_1.csv\"\nVAL_CSV = \"/kaggle/input/final-version-dataset/validation_data.csv\"\nTR_IMAGE_DIR = \"/kaggle/input/final-version-dataset/Dataset/Dataset/Train/\"\nVAL_IMAGE_DIR = \"/kaggle/input/final-version-dataset/Dataset/Dataset/Validation/\"\nTEACHER_PATH = \"openai/clip-vit-large-patch14-336\"\nBATCH_SIZE = 16\nNUM_CLASSES = 4\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T04:37:47.694790Z","iopub.execute_input":"2025-01-03T04:37:47.695489Z","iopub.status.idle":"2025-01-03T04:37:47.766507Z","shell.execute_reply.started":"2025-01-03T04:37:47.695454Z","shell.execute_reply":"2025-01-03T04:37:47.765372Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_CSV)\nval_df = pd.read_csv(VAL_CSV)\n\nprint(\"Unique values for Train: \" + str({**Counter(train_df.text)}))\nprint(\"Unique values for Validation: \" + str({**Counter(val_df.text)}))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T04:37:50.019491Z","iopub.execute_input":"2025-01-03T04:37:50.019831Z","iopub.status.idle":"2025-01-03T04:37:50.048765Z","shell.execute_reply.started":"2025-01-03T04:37:50.019801Z","shell.execute_reply":"2025-01-03T04:37:50.047758Z"}},"outputs":[{"name":"stdout","text":"Unique values for Train: {'Other': 299, 'Belkart': 216, 'Mastercard Other': 201, 'Mastercard Belkart Other': 201, 'Mastercard': 213, 'Visa Mastercard Belkart': 200, 'Visa Mastercard Other': 226, 'Visa Other': 201, 'Visa': 213, 'Belkart Other': 201}\nUnique values for Validation: {'Visa Mastercard Other': 101, 'Visa Mastercard': 100, 'Visa Mastercard Belkart': 200, 'Other': 202}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\"\"\"\"train_df = train_df[~((train_df.text == \"Mastercard Other\") | \n                      (train_df.text == \"Visa Other\") | \n                      (train_df.text == \"Other\") | \n                      (train_df.text == \"Belkart Other\"))]\n\ntrain_df.reset_index(drop=True, inplace=True)\nval_df.reset_index(drop=True, inplace=True)\"\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T04:02:30.368285Z","iopub.execute_input":"2025-01-03T04:02:30.368657Z","iopub.status.idle":"2025-01-03T04:02:30.377545Z","shell.execute_reply.started":"2025-01-03T04:02:30.368627Z","shell.execute_reply":"2025-01-03T04:02:30.376724Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_df.loc[:, \"labels\"] = train_df.labels.apply(ast.literal_eval)\ntrain_df.loc[:, \"labels\"] = train_df.labels.apply(np.float32)\n\nval_df.loc[:, \"labels\"] = val_df.labels.apply(ast.literal_eval)\nval_df.loc[:, \"labels\"] = val_df.labels.apply(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T04:37:53.312906Z","iopub.execute_input":"2025-01-03T04:37:53.313248Z","iopub.status.idle":"2025-01-03T04:37:53.355251Z","shell.execute_reply.started":"2025-01-03T04:37:53.313219Z","shell.execute_reply":"2025-01-03T04:37:53.354334Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Loader(Dataset):\n    def __init__(self, data_dir, df, processor):\n        super().__init__()\n        self.data_dir = data_dir\n        self.df = df\n        self.mean = np.append(np.array(processor.image_processor.image_mean),0.45)\n        self.std = np.append(np.array(processor.image_processor.image_std),0.26)\n\n    \n    def __len__(self):\n        return len(self.df)\n\n    def image_preprocessor(self, image):\n        image = image.resize((336,336))\n        image = np.array(image) / 255\n        image = image.transpose(2, 0, 1)\n        image = (image - self.mean[:, None, None]) / self.std[:, None, None]\n        return torch.tensor(image)\n\n    def __getitem__(self, index):\n        file_name = self.df[\"file_name\"][index]\n        labels = self.df[\"labels\"][index]\n        image = Image.open(self.data_dir + file_name).convert(\"RGBA\")\n        pixel_values = self.image_preprocessor(image)\n            \n        return [pixel_values.squeeze(0),\n                torch.tensor(labels)] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:15:00.196542Z","iopub.execute_input":"2025-01-03T05:15:00.197437Z","iopub.status.idle":"2025-01-03T05:15:00.204691Z","shell.execute_reply.started":"2025-01-03T05:15:00.197405Z","shell.execute_reply":"2025-01-03T05:15:00.203827Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class_list = [\n    \"Visa\",\n    \"Mastercard\",\n    \"Belkart\",\n    \"Other\"\n]\n\nprompt_list = []\nfor item in class_list:\n    prompt = \"A photo of \" + item\n    prompt_list.append(prompt)\n    \nprocessor = CLIPProcessor.from_pretrained(TEACHER_PATH)\n\ntext = processor.tokenizer(prompt_list, padding=\"max_length\",\n                           truncation=True, return_tensors=\"pt\").to(device)\n\ntrain_dataset = Loader(data_dir=TR_IMAGE_DIR, df=train_df,\n                       processor=processor)\n\nval_dataset = Loader(data_dir=VAL_IMAGE_DIR, df=val_df,\n                     processor=processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:15:02.429990Z","iopub.execute_input":"2025-01-03T05:15:02.430740Z","iopub.status.idle":"2025-01-03T05:15:03.170794Z","shell.execute_reply.started":"2025-01-03T05:15:02.430707Z","shell.execute_reply":"2025-01-03T05:15:03.169820Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, \n                              batch_size=BATCH_SIZE,\n                              shuffle=True)\n\neval_dataloader = DataLoader(val_dataset, \n                             batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:15:05.460052Z","iopub.execute_input":"2025-01-03T05:15:05.460955Z","iopub.status.idle":"2025-01-03T05:15:05.466205Z","shell.execute_reply.started":"2025-01-03T05:15:05.460908Z","shell.execute_reply":"2025-01-03T05:15:05.465136Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\"\"\"\"for param in CLIP.named_parameters():\n    if (param[0] == 'visual_projection.weight' or\n        param[0] == 'text_projection.weight' or\n        param[0] == \"logit_scale\"):\n        continue\n    else:\n        param[1].requires_grad=True\"\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T18:32:05.939022Z","iopub.execute_input":"2024-12-13T18:32:05.939446Z","iopub.status.idle":"2024-12-13T18:32:05.947130Z","shell.execute_reply.started":"2024-12-13T18:32:05.939410Z","shell.execute_reply":"2024-12-13T18:32:05.945741Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"def normalize(vector, e=1e-08):\n    return (vector - vector.mean())/(vector.std() + e)\n\nclass CustomModel(nn.Module):\n    def __init__(self, model, num_labels):\n        super(CustomModel,self).__init__()\n        self.num_labels = num_labels\n        self.model = model\n        self.dense = nn.Linear(4,512)\n        self.classifier = nn.Linear(512,4)\n        self.normalize = normalize\n        self.activation = nn.Sigmoid()\n    \n\n    def forward(self, input_ids, pixel_values, attention_mask, labels=None):\n\n        outputs = self.model(pixel_values=pixel_values,\n                             input_ids=input_ids,\n                             attention_mask=attention_mask)\n\n        logits_per_image = outputs.logits_per_image\n        #normalized_logits = self.normalize(logits_per_image)\n        flattened_logits = torch.flatten(logits_per_image)\n        pred_logits = self.dense(flattened_logits)\n        logits = self.activation(self.classifier(pred_logits))\n        \n        loss=None\n        if labels is not None:\n            criterion = nn.BCELoss()\n            loss = criterion(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n\n    \n        return SequenceClassifierOutput(loss=loss, logits=logits)\n\nvision_config = CLIPVisionConfig(num_channels=4, image_size=336, patch_size=14)\nconfiguration = CLIPConfig.from_text_vision_configs(CLIPTextConfig(), vision_config)\nCLIP = CLIPModel(configuration)\nmodel = CustomModel(CLIP, NUM_CLASSES).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:15:10.327335Z","iopub.execute_input":"2025-01-03T05:15:10.328158Z","iopub.status.idle":"2025-01-03T05:15:12.957216Z","shell.execute_reply.started":"2025-01-03T05:15:10.328125Z","shell.execute_reply":"2025-01-03T05:15:12.956533Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.999)\nf1 = evaluate.load(\"f1\", trust_remote_code=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:15:16.320847Z","iopub.execute_input":"2025-01-03T05:15:16.321184Z","iopub.status.idle":"2025-01-03T05:15:16.777855Z","shell.execute_reply.started":"2025-01-03T05:15:16.321154Z","shell.execute_reply":"2025-01-03T05:15:16.777187Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#iter = []\npreds = []\n\nfor epoch in range(5):\n    total_loss = 0.0\n    model.train()\n    for images,labels in tqdm(train_dataloader):\n        batch_loss = 0.0\n        for image, label in zip(images,labels):\n            image = image.to(device).unsqueeze(0)\n            label = label.to(device).unsqueeze(0)\n            \n            outputs = model(pixel_values=image,\n                           input_ids=text[\"input_ids\"],\n                           attention_mask=text[\"attention_mask\"],\n                           labels=label)\n        \n            loss = outputs.loss\n            batch_loss+=loss\n            \n        batch_loss.backward()\n        #plot_grad_flow(CLIP.named_parameters())\n        #iter.append(ave_grads)\n        optimizer.step()\n        optimizer.zero_grad()\n        total_loss += batch_loss.item()\n\n    model.eval()\n    for images,labels in tqdm(eval_dataloader):\n        for image,label in zip(images,labels):\n            image = image.to(device).unsqueeze(0)\n            label = label.to(device)\n\n            outputs = model(pixel_values=image,\n                            input_ids=text[\"input_ids\"],\n                            attention_mask=text[\"attention_mask\"])\n\n            activation = outputs.logits\n            predictions = (activation >= 0.5).float()\n            preds.append(predictions)\n            f1.add_batch(predictions=predictions,\n                         references=label)\n\n        \n    print(\"Training Loss: \" + str(total_loss / len(train_dataloader)))\n    print(f1.compute(average='binary'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:15:20.090558Z","iopub.execute_input":"2025-01-03T05:15:20.091120Z","iopub.status.idle":"2025-01-03T05:35:18.880406Z","shell.execute_reply.started":"2025-01-03T05:15:20.091088Z","shell.execute_reply":"2025-01-03T05:35:18.879156Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/136 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6aded36be8124ba6ae0a9e63c134d679"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/38 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5e7d2e851f3473f81962a60a7513d4a"}},"metadata":{}},{"name":"stdout","text":"Training Loss: 10.37223939334645\n{'f1': 0.7138286526839673}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/136 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d461f80986b44eef8111e65b8d883a71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/38 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f158266b07774414be6330e197677d41"}},"metadata":{}},{"name":"stdout","text":"Training Loss: 8.650934892542223\n{'f1': 0.5607327757865394}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/136 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1271284af8b74633a7928eccc5638078"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/38 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e9d6cf630f94789a9551bdb14c0aa41"}},"metadata":{}},{"name":"stdout","text":"Training Loss: 8.46806132442811\n{'f1': 0.5842323651452282}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/136 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67809cb90d3e4afd9207f84e4dc0f925"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     19\u001b[0m     batch_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\n\u001b[0;32m---> 21\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#plot_grad_flow(CLIP.named_parameters())\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#iter.append(ave_grads)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"#pred = torch.cat(preds, dim=-1)\npred = torch.stack(preds)\ntrue = torch.tensor(val_df[\"labels\"], dtype=torch.float32)\n\nprint(sklearn.metrics.classification_report(\n    true.cpu().numpy(),\n    pred[-true.shape[0]:].cpu().numpy(),\n    target_names=[\"Mastercard\",\n                  \"Visa\",\n                  \"Белкарт\",\n                  \"Иные\"]\n))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:12:29.044790Z","iopub.execute_input":"2025-01-03T05:12:29.045490Z","iopub.status.idle":"2025-01-03T05:12:29.070933Z","shell.execute_reply.started":"2025-01-03T05:12:29.045458Z","shell.execute_reply":"2025-01-03T05:12:29.069992Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n  Mastercard       1.00      0.25      0.40       401\n        Visa       0.00      0.00      0.00       401\n     Белкарт       0.00      0.00      0.00       200\n        Иные       0.40      0.67      0.50       303\n\n   micro avg       0.33      0.23      0.27      1305\n   macro avg       0.35      0.23      0.23      1305\nweighted avg       0.40      0.23      0.24      1305\n samples avg       0.25      0.39      0.29      1305\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/1225409758.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n  true = torch.tensor(val_df[\"labels\"], dtype=torch.float32)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"image = train_dataset[830][0].unsqueeze(0).to(device)\ntext = processor.tokenizer(text=[\"a photo of Visa\",\"a photo of Mastercard\", \"a photo of Belkart\",\n                                 \"a photo of Other\"],\n                          return_tensors=\"pt\",\n                          padding=True).to(device)\n\noutputs = CLIP(pixel_values=image, input_ids=text[\"input_ids\"])\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=-1)\nprint(torch.round(probs, decimals=3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:27:37.690634Z","iopub.execute_input":"2024-12-30T06:27:37.691509Z","iopub.status.idle":"2024-12-30T06:27:37.746261Z","shell.execute_reply.started":"2024-12-30T06:27:37.691458Z","shell.execute_reply":"2024-12-30T06:27:37.745278Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.1450, 0.1830, 0.2330, 0.4390]], device='cuda:0',\n       grad_fn=<RoundBackward1>)\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"train_df[train_df.text == \"Other\"].index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:38:53.948795Z","iopub.execute_input":"2025-01-03T03:38:53.949156Z","iopub.status.idle":"2025-01-03T03:38:53.956279Z","shell.execute_reply.started":"2025-01-03T03:38:53.949128Z","shell.execute_reply":"2025-01-03T03:38:53.955379Z"}},"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       919, 920, 921, 922, 923, 924, 925, 926, 927, 928],\n      dtype='int64', length=299)"},"metadata":{}}],"execution_count":73},{"cell_type":"code","source":"val_df[val_df.text == \"Visa Mastercard Other\"].index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:04:20.330620Z","iopub.execute_input":"2024-12-30T06:04:20.330934Z","iopub.status.idle":"2024-12-30T06:04:20.338048Z","shell.execute_reply.started":"2024-12-30T06:04:20.330908Z","shell.execute_reply":"2024-12-30T06:04:20.337016Z"}},"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n        91,  92,  93,  94,  95,  96,  97,  98,  99, 100],\n      dtype='int64', length=101)"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"image = val_dataset[95][0].unsqueeze(0).to(device)\n# for reference: Mastercard, Visa, Belkart, Other\n\noutputs = model(pixel_values=image, \n                input_ids=text[\"input_ids\"],\n                attention_mask=text[\"attention_mask\"])\npreds = outputs.logits\nprint(torch.round(preds, decimals=3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:28:00.954607Z","iopub.execute_input":"2024-12-30T06:28:00.954967Z","iopub.status.idle":"2024-12-30T06:28:01.010909Z","shell.execute_reply.started":"2024-12-30T06:28:00.954923Z","shell.execute_reply":"2024-12-30T06:28:01.010004Z"}},"outputs":[{"name":"stdout","text":"tensor([0.9770, 0.1490, 0.9130, 0.8120], device='cuda:0',\n       grad_fn=<RoundBackward1>)\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"\"\"\"def plot_grad_flow(named_parameters):\n    global ave_grads\n    ave_grads = []\n    global layers\n    layers = []\n    for n, p in named_parameters:\n        if(p.requires_grad) and (\"bias\" not in n):\n            layers.append(n)\n            ave_grads.append(p.grad.abs().mean().cpu())\n    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n    plt.xlim(xmin=0, xmax=len(ave_grads))\n    plt.xlabel(\"Layers\")\n    plt.ylabel(\"average gradient\")\n    plt.title(\"Gradient flow\")\n    plt.grid(True)\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T18:32:11.528034Z","iopub.execute_input":"2024-12-13T18:32:11.528786Z","iopub.status.idle":"2024-12-13T18:32:11.535098Z","shell.execute_reply.started":"2024-12-13T18:32:11.528748Z","shell.execute_reply":"2024-12-13T18:32:11.534033Z"}},"outputs":[],"execution_count":102}]}