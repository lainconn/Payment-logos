{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10192974,"sourceType":"datasetVersion","datasetId":6297924}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate\n\nimport pandas as pd\nfrom collections import Counter\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport transformers\nfrom transformers import (\n    CLIPProcessor,\n    CLIPModel\n)\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom PIL import Image\nfrom tqdm import tqdm\nimport evaluate\nimport ast\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T22:25:55.237664Z","iopub.execute_input":"2024-12-15T22:25:55.237991Z","iopub.status.idle":"2024-12-15T22:26:21.941475Z","shell.execute_reply.started":"2024-12-15T22:25:55.237959Z","shell.execute_reply":"2024-12-15T22:26:21.940779Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"TRAIN_CSV = \"/kaggle/input/simplified-task/train_data.csv\"\nVAL_CSV = \"/kaggle/input/simplified-task/validation_data.csv\"\nTR_IMAGE_DIR = \"/kaggle/input/simplified-task/Dataset/Dataset/Dataset/Train/\"\nVAL_IMAGE_DIR = \"/kaggle/input/simplified-task/Dataset/Dataset/Dataset/Validation/\"\nTEACHER_PATH = \"openai/clip-vit-base-patch32\"\nBATCH_SIZE = 32\nNUM_CLASSES = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T22:26:55.198368Z","iopub.execute_input":"2024-12-15T22:26:55.199076Z","iopub.status.idle":"2024-12-15T22:26:55.203538Z","shell.execute_reply.started":"2024-12-15T22:26:55.199010Z","shell.execute_reply":"2024-12-15T22:26:55.202717Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_CSV)\nval_df = pd.read_csv(VAL_CSV)\n\nprint(\"Unique values for Train: \" + str({**Counter(train_df.text)}))\nprint(\"Unique values for Validation: \" + str({**Counter(val_df.text)}))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T22:26:57.321403Z","iopub.execute_input":"2024-12-15T22:26:57.322265Z","iopub.status.idle":"2024-12-15T22:26:57.362059Z","shell.execute_reply.started":"2024-12-15T22:26:57.322224Z","shell.execute_reply":"2024-12-15T22:26:57.361269Z"}},"outputs":[{"name":"stdout","text":"Unique values for Train: {'Belkart': 216, 'Mastercard': 213, 'Mir': 202, 'Visa Mastercard Mir': 226, 'Visa': 213, 'Visa Mastercard Belkart': 200, 'Mastercard Belkart Mir': 201, 'Visa Mir': 201, 'Accept': 190, 'Belkart Password': 201, 'ID-Check Belkart Password': 201, 'ID-Check Belkart': 200, 'ID-Check Mastercard': 201, 'ID-Check': 198, 'Mir Accept': 201, 'Password': 198, 'Secure ID-Check': 201, 'Secure': 202, 'Visa Secure': 201, 'Other': 201}\nUnique values for Validation: {'Visa Mastercard Mir': 101, 'Visa Mastercard': 100, 'Visa Mastercard Belkart': 200, 'ID-Check Accept Secure Password': 101, 'ID-Check Secure Password': 101}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train_df = train_df[(train_df.text == \"Visa Mir\") | \n                    (train_df.text == \"Visa\") | \n                    (train_df.text == \"Other\") |\n                    (train_df.text == \"Mir\") | \n                    (train_df.text == \"Mastercard\") |\n                    (train_df.text == \"Visa Mastercard Mir\")]\n\nval_df = val_df[(val_df.text == \"Visa Mastercard Mir\") |\n                (val_df.text == \"Visa Mastercard\") |\n                (val_df.text == \"Visa Mastercard Belkart\")]\n\ntrain_df.reset_index(drop=True, inplace=True)\nval_df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T22:27:00.317082Z","iopub.execute_input":"2024-12-15T22:27:00.317817Z","iopub.status.idle":"2024-12-15T22:27:00.329889Z","shell.execute_reply.started":"2024-12-15T22:27:00.317780Z","shell.execute_reply":"2024-12-15T22:27:00.329136Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_df.loc[:, \"labels\"] = train_df.labels.apply(ast.literal_eval)\ntrain_df.loc[:, \"labels\"] = train_df.labels.apply(np.float32)\n\nval_df.loc[:, \"labels\"] = val_df.labels.apply(ast.literal_eval)\nval_df.loc[:, \"labels\"] = val_df.labels.apply(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T22:27:03.481133Z","iopub.execute_input":"2024-12-15T22:27:03.481833Z","iopub.status.idle":"2024-12-15T22:27:03.509018Z","shell.execute_reply.started":"2024-12-15T22:27:03.481801Z","shell.execute_reply":"2024-12-15T22:27:03.508066Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Loader(Dataset):\n    def __init__(self, data_dir, df, processor):\n        super().__init__()\n        self.data_dir = data_dir\n        self.df = df\n        self.processor = processor\n\n    \n    def __len__(self):\n        return len(self.df)\n\n\n    def __getitem__(self, index):\n        file_name = self.df[\"file_name\"][index]\n        labels = self.df[\"labels\"][index]\n        text = self.df[\"text\"][index]\n        image = Image.open(self.data_dir + file_name).convert(\"RGBA\")\n        pixel_values = self.processor.image_processor(image, return_tensors=\"pt\").pixel_values\n        input_ids = self.processor.tokenizer(text, padding=\"max_length\", truncation=True)\n        \n        return {\"pixel_values\": pixel_values.squeeze(),\n                \"input_ids\": torch.tensor(input_ids.input_ids),\n                \"attention_mask\": torch.tensor(input_ids.attention_mask),\n                \"labels\": torch.tensor(labels)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T22:27:57.347163Z","iopub.execute_input":"2024-12-15T22:27:57.347505Z","iopub.status.idle":"2024-12-15T22:27:57.353965Z","shell.execute_reply.started":"2024-12-15T22:27:57.347471Z","shell.execute_reply":"2024-12-15T22:27:57.353090Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"processor = CLIPProcessor.from_pretrained(TEACHER_PATH)\n\ntrain_dataset = Loader(data_dir=TR_IMAGE_DIR, df=train_df,\n                       processor=processor)\n\nval_dataset = Loader(data_dir=VAL_IMAGE_DIR, df=val_df,\n                      processor=processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T22:27:59.157856Z","iopub.execute_input":"2024-12-15T22:27:59.158245Z","iopub.status.idle":"2024-12-15T22:28:05.317930Z","shell.execute_reply.started":"2024-12-15T22:27:59.158207Z","shell.execute_reply":"2024-12-15T22:28:05.316772Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75d1c284de2f4df4999ce62755faa150"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96f058bf3d4f44668a1da8a5fa906fe3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"736fafd5d4e04d0593fdfa581d9636c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95f0b59d20b847e4988dc6e43f21fce5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"608ed54cbdb3474a8ef234b0ae7ccf3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0eb21b39e2c046b7ad0a3094eedab9bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d3b8bef48b04473a29e38baf060d549"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, \n                              batch_size=BATCH_SIZE,\n                              shuffle=True)\n\neval_dataloader = DataLoader(val_dataset, \n                             batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T22:28:07.777351Z","iopub.execute_input":"2024-12-15T22:28:07.778174Z","iopub.status.idle":"2024-12-15T22:28:07.782294Z","shell.execute_reply.started":"2024-12-15T22:28:07.778138Z","shell.execute_reply":"2024-12-15T22:28:07.781519Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nCLIP = CLIPModel.from_pretrained(TEACHER_PATH,attn_implementation=\"sdpa\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T22:28:10.468528Z","iopub.execute_input":"2024-12-15T22:28:10.468857Z","iopub.status.idle":"2024-12-15T22:28:36.220217Z","shell.execute_reply.started":"2024-12-15T22:28:10.468828Z","shell.execute_reply":"2024-12-15T22:28:36.219471Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"004d560e8b244ad08b8d39ca81e02cd0"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"for param in CLIP.named_parameters():\n    if (param[0] == 'visual_projection.weight' or\n        param[0] == 'text_projection.weight' or\n        param[0] == \"logit_scale\"):\n        continue\n    else:\n        param[1].requires_grad=True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T18:32:05.939022Z","iopub.execute_input":"2024-12-13T18:32:05.939446Z","iopub.status.idle":"2024-12-13T18:32:05.947130Z","shell.execute_reply.started":"2024-12-13T18:32:05.939410Z","shell.execute_reply":"2024-12-13T18:32:05.945741Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"\"\"\"\"optimizer = torch.optim.SGD(CLIP.parameters(),\n                            lr=1e-4,\n                            momentum=0.999)\"\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T04:32:35.162930Z","iopub.execute_input":"2024-12-08T04:32:35.163243Z","iopub.status.idle":"2024-12-08T04:32:35.168854Z","shell.execute_reply.started":"2024-12-08T04:32:35.163218Z","shell.execute_reply":"2024-12-08T04:32:35.167884Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(CLIP.parameters(),\n                              lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T18:32:08.783479Z","iopub.execute_input":"2024-12-13T18:32:08.783881Z","iopub.status.idle":"2024-12-13T18:32:08.790338Z","shell.execute_reply.started":"2024-12-13T18:32:08.783848Z","shell.execute_reply":"2024-12-13T18:32:08.789336Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"def plot_grad_flow(named_parameters):\n    global ave_grads\n    ave_grads = []\n    global layers\n    layers = []\n    for n, p in named_parameters:\n        if(p.requires_grad) and (\"bias\" not in n):\n            layers.append(n)\n            ave_grads.append(p.grad.abs().mean().cpu())\n    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n    plt.xlim(xmin=0, xmax=len(ave_grads))\n    plt.xlabel(\"Layers\")\n    plt.ylabel(\"average gradient\")\n    plt.title(\"Gradient flow\")\n    plt.grid(True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T18:32:11.528034Z","iopub.execute_input":"2024-12-13T18:32:11.528786Z","iopub.status.idle":"2024-12-13T18:32:11.535098Z","shell.execute_reply.started":"2024-12-13T18:32:11.528748Z","shell.execute_reply":"2024-12-13T18:32:11.534033Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"iter = []\nfor epoch in range(5):\n    train_loss = 0.0\n    CLIP.train()\n    for batch in tqdm(train_dataloader):\n        for k,v in batch.items():\n            batch[k] = v.to(device)\n            \n        outputs = CLIP(pixel_values=batch[\"pixel_values\"],\n                       input_ids=batch[\"input_ids\"],\n                       attention_mask=batch[\"attention_mask\"],\n                       return_loss=True)\n\n        loss = outputs.loss\n        loss.backward()\n        plot_grad_flow(CLIP.named_parameters())\n        iter.append(ave_grads)\n        optimizer.step()\n        optimizer.zero_grad()\n        train_loss += loss.item()\n\n    \"\"\"CLIP.eval()\n    eval_loss=0.0\n    for batch in tqdm(eval_dataloader):\n        for k,v in batch.items():\n            batch[k] = v.to(device)\n\n        outputs = CLIP(pixel_values=batch[\"pixel_values\"],\n                      input_ids=batch[\"input_ids\"],\n                      attention_mask=batch[\"attention_mask\"],\n                      return_loss=True)\n\n        loss=outputs.loss\n        eval_loss+=loss.item()\"\"\"\n\n        \n    print(\"Training Loss: \" + str(train_loss / len(train_dataloader)))\n    #print(\"Validation Loss: \" + str(eval_loss / len(eval_dataloader)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T18:32:13.530417Z","iopub.execute_input":"2024-12-13T18:32:13.530809Z","iopub.status.idle":"2024-12-13T18:34:38.340675Z","shell.execute_reply.started":"2024-12-13T18:32:13.530779Z","shell.execute_reply":"2024-12-13T18:34:38.339781Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 3.351712906360626\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:29<00:00,  1.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 2.1245479017496107\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:28<00:00,  1.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.8038956582546235\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:28<00:00,  1.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.7901268899440765\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:28<00:00,  1.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.763461822271347\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":103},{"cell_type":"code","source":"train_df[train_df.text == \"Visa.Mir\"].index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T18:36:12.555806Z","iopub.execute_input":"2024-12-13T18:36:12.556835Z","iopub.status.idle":"2024-12-13T18:36:12.566671Z","shell.execute_reply.started":"2024-12-13T18:36:12.556794Z","shell.execute_reply":"2024-12-13T18:36:12.565505Z"}},"outputs":[{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"Index([ 854,  855,  856,  857,  858,  859,  860,  861,  862,  863,\n       ...\n       1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054],\n      dtype='int64', length=201)"},"metadata":{}}],"execution_count":110},{"cell_type":"code","source":"image = train_dataset[1051][\"pixel_values\"].unsqueeze(0).to(device)\ntext = processor.tokenizer(text=[\"a photo of Visa\",\"a photo of Mastercard\", \"a photo of Mir\",\n                                 \"a photo of Other\"],\n                          return_tensors=\"pt\",\n                          padding=True).to(device)\n\noutputs = CLIP(pixel_values=image, input_ids=text[\"input_ids\"])\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=-1)\nprint(torch.round(probs, decimals=3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T18:36:17.019812Z","iopub.execute_input":"2024-12-13T18:36:17.020193Z","iopub.status.idle":"2024-12-13T18:36:17.070974Z","shell.execute_reply.started":"2024-12-13T18:36:17.020160Z","shell.execute_reply":"2024-12-13T18:36:17.069991Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.0010, 0.0000, 0.8870, 0.1120]], device='cuda:0',\n       grad_fn=<RoundBackward1>)\n","output_type":"stream"}],"execution_count":111},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, model, num_labels):\n        super(CustomModel,self).__init__()\n        self.num_labels = num_labels\n        self.model = model\n        self.classifier = nn.Linear(model.vision_model.config.hidden_size,num_labels)\n    \n\n    def forward(self, pixel_values, labels=None):\n\n        outputs = self.model.vision_model(pixel_values=pixel_values,\n                                          output_hidden_states=True)\n\n        pooler_output = outputs.pooler_output\n        logits = self.classifier(pooler_output)          \n        \n        loss=None\n        if labels is not None:\n            criterion = nn.BCEWithLogitsLoss()\n            loss = criterion(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n\n    \n        return SequenceClassifierOutput(loss=loss, logits=logits)\n\nmodel = CustomModel(CLIP, NUM_CLASSES).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T22:28:51.698475Z","iopub.execute_input":"2024-12-15T22:28:51.698805Z","iopub.status.idle":"2024-12-15T22:28:51.711072Z","shell.execute_reply.started":"2024-12-15T22:28:51.698776Z","shell.execute_reply":"2024-12-15T22:28:51.710217Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nf1 = evaluate.load(\"f1\", trust_remote_code=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T22:28:54.590978Z","iopub.execute_input":"2024-12-15T22:28:54.591381Z","iopub.status.idle":"2024-12-15T22:28:56.356245Z","shell.execute_reply.started":"2024-12-15T22:28:54.591351Z","shell.execute_reply":"2024-12-15T22:28:56.355421Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f91cf033a74489dabee0e61788835b3"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"preds = []\n\nfor epoch in range(5):\n    model.train()\n    train_loss=0.0\n    for batch in tqdm(train_dataloader):\n        for k,v in batch.items():\n            batch[k] = v.to(device)\n            \n        outputs = model(pixel_values=batch[\"pixel_values\"],\n                        labels=batch[\"labels\"])\n        \n        loss = outputs.loss\n        train_loss+=loss.item()\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader):\n            for k,v in batch.items():\n                batch[k] = v.to(device)\n         \n            outputs = model(pixel_values=batch[\"pixel_values\"])\n\n            logits = outputs.logits\n            activation = torch.sigmoid(logits)\n            predictions = (activation >= 0.5).float()\n            preds.append(predictions)\n            for indx in range(activation.shape[0]):\n                f1.add_batch(predictions=predictions[indx],\n                             references=batch[\"labels\"][indx])\n        \n    \n    print(\"Training Loss: \" + str(train_loss / len(train_dataloader)))     \n    print(f1.compute(average='binary'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T22:28:59.900196Z","iopub.execute_input":"2024-12-15T22:28:59.900899Z","iopub.status.idle":"2024-12-15T22:30:41.968564Z","shell.execute_reply.started":"2024-12-15T22:28:59.900868Z","shell.execute_reply":"2024-12-15T22:30:41.967053Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 40/40 [00:28<00:00,  1.40it/s]\n100%|██████████| 13/13 [00:10<00:00,  1.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.17569373662117868\n{'f1': 0.2659574468085107}\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:19<00:00,  2.07it/s]\n100%|██████████| 13/13 [00:07<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.011339819460408762\n{'f1': 0.2659574468085107}\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:19<00:00,  2.08it/s]\n100%|██████████| 13/13 [00:07<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.004114532720996067\n{'f1': 0.2659574468085107}\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 18/40 [00:09<00:11,  1.93it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      8\u001b[0m     batch[k] \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpixel_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     14\u001b[0m train_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mCustomModel.forward\u001b[0;34m(self, pixel_values, labels)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     pooler_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[1;32m     15\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(pooler_output)          \n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:1097\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m   1094\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding)\n\u001b[1;32m   1095\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m-> 1097\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1105\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:877\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    869\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    870\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    871\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    874\u001b[0m         output_attentions,\n\u001b[1;32m    875\u001b[0m     )\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:608\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    605\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    607\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(hidden_states)\n\u001b[0;32m--> 608\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    616\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:526\u001b[0m, in \u001b[0;36mCLIPSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    524\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m    525\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m--> 526\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    529\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"pred = torch.cat(preds, dim=0)\ntrue = torch.tensor(val_df[\"labels\"], dtype=torch.float32)\n\nprint(sklearn.metrics.classification_report(\n    true.cpu().numpy(),\n    pred[-true.shape[0]:].cpu().numpy(),\n    target_names=[\"Mastercard\",\n                  \"Visa\",\n                  \"Мир\",\n                  \"Иные\"]\n))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T22:31:08.471508Z","iopub.execute_input":"2024-12-15T22:31:08.472110Z","iopub.status.idle":"2024-12-15T22:31:08.494340Z","shell.execute_reply.started":"2024-12-15T22:31:08.472069Z","shell.execute_reply":"2024-12-15T22:31:08.493430Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n  Mastercard       0.00      0.00      0.00       401\n        Visa       0.00      0.00      0.00       401\n         Мир       0.00      0.00      0.00       101\n        Иные       0.50      1.00      0.67       200\n\n   micro avg       0.50      0.18      0.27      1103\n   macro avg       0.12      0.25      0.17      1103\nweighted avg       0.09      0.18      0.12      1103\n samples avg       0.50      0.17      0.25      1103\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/4210245038.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n  true = torch.tensor(val_df[\"labels\"], dtype=torch.float32)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":15}]}
